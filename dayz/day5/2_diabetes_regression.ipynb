{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Dataset - Comprehensive Regression Analysis\n",
    "\n",
    "This notebook analyzes the Diabetes dataset using various regression algorithms to predict disease progression.\n",
    "\n",
    "## Dataset Overview\n",
    "- **Target**: Quantitative measure of disease progression one year after baseline\n",
    "- **Features**: 10 baseline variables (age, sex, BMI, blood pressure, and 6 blood serum measurements)\n",
    "- **Samples**: 442 diabetes patients\n",
    "- **Type**: Medical/Healthcare data\n",
    "\n",
    "## Models to Compare:\n",
    "1. Linear Regression\n",
    "2. Ridge Regression\n",
    "3. Lasso Regression\n",
    "4. Decision Tree Regressor (DTR)\n",
    "5. Random Forest Regressor (RFR)\n",
    "6. Support Vector Regressor (SVR)\n",
    "7. Gradient Boosting Regressor\n",
    "8. XGBoost Regressor\n",
    "9. LightGBM Regressor\n",
    "10. Elastic Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "feature_names = diabetes.feature_names\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Features: {list(feature_names)}\")\n",
    "print(f\"Target: Disease progression measure\")\n",
    "print(f\"Data type: {diabetes.DESCR.split('\\\\n')[0]}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset description and EDA\n",
    "print(\"Dataset Description:\")\n",
    "print(diabetes.DESCR[:1000] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Dataset Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nTarget variable range: {y.min():.2f} to {y.max():.2f}\")\n",
    "print(f\"Target variable mean: {y.mean():.2f}\")\n",
    "print(f\"Target variable std: {y.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution analysis\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(df['target'], bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "plt.title('Target Distribution (Disease Progression)')\n",
    "plt.xlabel('Disease Progression Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot(df['target'])\n",
    "plt.title('Target Boxplot')\n",
    "plt.ylabel('Disease Progression Score')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "df['target'].plot(kind='kde', alpha=0.7, color='darkblue')\n",
    "plt.title('Target Density Plot')\n",
    "plt.xlabel('Disease Progression Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, fmt='.3f')\n",
    "plt.title('Correlation Matrix - Diabetes Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature correlation with target\n",
    "target_corr = correlation_matrix['target'].sort_values(ascending=False)\n",
    "print(\"Feature correlation with disease progression:\")\n",
    "print(target_corr)\n",
    "\n",
    "# Top correlated features\n",
    "print(f\"\\nTop positive correlations:\")\n",
    "positive_corr = target_corr[target_corr > 0].drop('target')\n",
    "for feature, corr in positive_corr.head(3).items():\n",
    "    print(f\"  {feature}: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\nTop negative correlations:\")\n",
    "negative_corr = target_corr[target_corr < 0]\n",
    "for feature, corr in negative_corr.head(3).items():\n",
    "    print(f\"  {feature}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, feature in enumerate(feature_names, 1):\n",
    "    plt.subplot(3, 4, i)\n",
    "    plt.hist(df[feature], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    plt.title(f'{feature}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"Target range: {y.min():.2f} - {y.max():.2f}\")\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Evaluate a regression model and return metrics\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Train MAE': train_mae,\n",
    "        'Test MAE': test_mae,\n",
    "        'Train R²': train_r2,\n",
    "        'Test R²': test_r2,\n",
    "        'CV R² Mean': cv_mean,\n",
    "        'CV R² Std': cv_std\n",
    "    }, y_test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso Regression': Lasso(alpha=1.0, random_state=42),\n",
    "    'Elastic Net': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(random_state=42, verbose=-1),\n",
    "    'SVR': SVR(kernel='rbf')\n",
    "}\n",
    "\n",
    "# Models that need scaled data\n",
    "scaled_models = ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'Elastic Net', 'SVR']\n",
    "\n",
    "print(\"Models to be evaluated:\")\n",
    "for i, model_name in enumerate(models.keys(), 1):\n",
    "    print(f\"{i:2d}. {model_name}\")\n",
    "    \n",
    "print(f\"\\nNote: {len(scaled_models)} models will use scaled features\")\n",
    "print(f\"      {len(models) - len(scaled_models)} models will use original features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results_list = []\n",
    "predictions_dict = {}\n",
    "\n",
    "print(\"Training and evaluating models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Choose scaled or unscaled data\n",
    "    if model_name in scaled_models:\n",
    "        X_train_use = X_train_scaled\n",
    "        X_test_use = X_test_scaled\n",
    "    else:\n",
    "        X_train_use = X_train\n",
    "        X_test_use = X_test\n",
    "    \n",
    "    # Evaluate model\n",
    "    results, y_pred = evaluate_model(model, X_train_use, X_test_use, y_train, y_test, model_name)\n",
    "    results_list.append(results)\n",
    "    predictions_dict[model_name] = y_pred\n",
    "    \n",
    "    print(f\"✓ {model_name} - Test R²: {results['Test R²']:.4f}, Test RMSE: {results['Test RMSE']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All models trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results analysis\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "# Sort by Test R² score (descending)\n",
    "results_df = results_df.sort_values('Test R²', ascending=False)\n",
    "\n",
    "print(\"Model Performance Comparison - Diabetes Dataset:\")\n",
    "print(\"=\" * 90)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Best performing model\n",
    "best_model = results_df.iloc[0]['Model']\n",
    "best_r2 = results_df.iloc[0]['Test R²']\n",
    "best_rmse = results_df.iloc[0]['Test RMSE']\n",
    "\n",
    "print(f\"\\n🏆 Best performing model: {best_model}\")\n",
    "print(f\"   Test R² = {best_r2:.4f}\")\n",
    "print(f\"   Test RMSE = {best_rmse:.2f}\")\n",
    "\n",
    "# Model performance insights\n",
    "print(f\"\\n📊 Performance Insights:\")\n",
    "print(f\"   • Best R² Score: {results_df['Test R²'].max():.4f}\")\n",
    "print(f\"   • Worst R² Score: {results_df['Test R²'].min():.4f}\")\n",
    "print(f\"   • Performance Range: {results_df['Test R²'].max() - results_df['Test R²'].min():.4f}\")\n",
    "print(f\"   • Average R² Score: {results_df['Test R²'].mean():.4f}\")\n",
    "print(f\"   • Standard Deviation: {results_df['Test R²'].std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of model performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# R² Score comparison\n",
    "axes[0, 0].barh(results_df['Model'], results_df['Test R²'], color='lightblue')\n",
    "axes[0, 0].set_xlabel('Test R² Score')\n",
    "axes[0, 0].set_title('Model Performance Comparison (R² Score)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0, 1].barh(results_df['Model'], results_df['Test RMSE'], color='lightcoral')\n",
    "axes[0, 1].set_xlabel('Test RMSE')\n",
    "axes[0, 1].set_title('Model Performance Comparison (RMSE)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-validation scores with error bars\n",
    "cv_means = results_df['CV R² Mean']\n",
    "cv_stds = results_df['CV R² Std']\n",
    "axes[1, 0].barh(results_df['Model'], cv_means, color='lightgreen', \n",
    "                xerr=cv_stds, capsize=5)\n",
    "axes[1, 0].set_xlabel('Cross-Validation R² Mean ± Std')\n",
    "axes[1, 0].set_title('Cross-Validation Performance')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE comparison\n",
    "axes[1, 1].barh(results_df['Model'], results_df['Test MAE'], color='gold')\n",
    "axes[1, 1].set_xlabel('Test MAE')\n",
    "axes[1, 1].set_title('Model Performance Comparison (MAE)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual plots for top 4 models\n",
    "top_4_models = results_df.head(4)['Model'].tolist()\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i, model_name in enumerate(top_4_models, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    y_pred = predictions_dict[model_name]\n",
    "    \n",
    "    plt.scatter(y_test, y_pred, alpha=0.6, s=30)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Disease Progression')\n",
    "    plt.ylabel('Predicted Disease Progression')\n",
    "    plt.title(f'{model_name}')\n",
    "    \n",
    "    # Add metrics to the plot\n",
    "    r2 = results_df[results_df['Model'] == model_name]['Test R²'].iloc[0]\n",
    "    rmse = results_df[results_df['Model'] == model_name]['Test RMSE'].iloc[0]\n",
    "    plt.text(0.05, 0.95, f'R² = {r2:.4f}\\\\nRMSE = {rmse:.2f}', \n",
    "             transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings - Diabetes Dataset\n",
    "\n",
    "### Model Performance Summary:\n",
    "- **Best Model**: The top-performing model demonstrates strong predictive capability for disease progression\n",
    "- **Dataset Characteristics**: Small dataset (442 samples) with 10 standardized features\n",
    "- **Feature Relationships**: BMI and blood pressure measurements show significant correlations\n",
    "- **Model Comparison**: Performance varies significantly across different algorithm types\n",
    "\n",
    "### Medical Insights:\n",
    "1. **Predictive Factors**: Certain baseline measurements are more predictive of disease progression\n",
    "2. **Model Interpretability**: Linear models provide clear coefficient interpretation for medical professionals\n",
    "3. **Robustness**: Cross-validation helps ensure model reliability for medical applications\n",
    "4. **Feature Standardization**: All features are pre-standardized in this dataset\n",
    "\n",
    "### Recommendations:\n",
    "1. **Clinical Use**: Consider the top-performing models for disease progression prediction\n",
    "2. **Interpretability vs Performance**: Balance between model accuracy and clinical interpretability\n",
    "3. **Validation**: Extensive validation needed before clinical deployment\n",
    "4. **Feature Engineering**: Consider interaction terms between biological markers\n",
    "\n",
    "### Next Steps:\n",
    "- Hyperparameter optimization for best models\n",
    "- Feature importance analysis for clinical insights\n",
    "- Ensemble methods for improved robustness\n",
    "- External validation on independent patient cohorts\n",
    "- Integration with clinical decision support systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
